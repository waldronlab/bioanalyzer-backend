from fastapi import FastAPI, WebSocket, HTTPException, Request, UploadFile, File, Form, Body, Query
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.websockets import WebSocketDisconnect
from fastapi.responses import RedirectResponse, JSONResponse
from pydantic import BaseModel
from typing import Dict, List, Optional
import json
import torch
from pathlib import Path
from datetime import datetime
import pytz
from app.models.config import ModelConfig
from app.models.unified_qa import UnifiedQA
from app.services.data_retrieval import PubMedRetriever
from app.utils.text_processing import AdvancedTextProcessor
from app.utils.config import (
    NCBI_API_KEY, 
    GEMINI_API_KEY, 
    DEFAULT_MODEL,
    AVAILABLE_MODELS
)
from app.utils.methods_scorer import MethodsScorer
import re
import asyncio
import logging
import sys
import os
from bs4 import BeautifulSoup
import csv
from app.services.cache_manager import CacheManager

# Add the project root to Python path
# sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="BioAnalyzer - BugSigDB Curation Analysis",
    description="""
    **BioAnalyzer** - A specialized tool for analyzing scientific papers for BugSigDB curation readiness.
    
    ## Core Functionality
    
    This API focuses on analyzing papers for **6 essential BugSigDB curation fields**:
    
    1. **Host Species** - What organism is being studied (e.g., Human, Mouse, Rat)
    2. **Body Site** - Where the microbiome sample was collected (e.g., Gut, Oral, Skin)
    3. **Condition** - What disease/treatment/exposure is being studied
    4. **Sequencing Type** - What molecular method was used (e.g., 16S, metagenomics)
    5. **Taxa Level** - What taxonomic level was analyzed (e.g., phylum, genus, species)
    6. **Sample Size** - Number of samples analyzed
    
    ## Analysis Results
    
    For each field, the system provides:
    - **Status**: PRESENT, PARTIALLY_PRESENT, or ABSENT
    - **Value**: The extracted information
    - **Confidence**: Confidence score (0.0-1.0)
    - **Reason if Missing**: Why the field is not present
    - **Suggestions**: What additional information is needed for curation
    
    ## Curation Readiness
    
    A paper is considered **ready for curation** when all 6 fields have status "PRESENT".
    
    ## Endpoints
    
    - **Paper Analysis**: Single and batch analysis of papers by PMID
    - **CSV Upload**: Batch processing of multiple PMIDs from CSV files
    - **Curation Statistics**: Performance metrics and field analysis statistics
    - **Cache Management**: Efficient storage and retrieval of analysis results
    """,
    version="1.0.0",
    contact={
        "name": "BioAnalyzer Team",
        "url": "https://github.com/your-repo/bioanalyzer",
    },
    license_info={
        "name": "MIT",
        "url": "https://opensource.org/licenses/MIT",
    },
    tags_metadata=[
        {
            "name": "Paper Analysis",
            "description": "Core endpoints for analyzing papers for BugSigDB curation readiness using the 6 essential fields."
        },
        {
            "name": "Batch Processing",
            "description": "Endpoints for processing multiple papers at once, including CSV uploads."
        },
        {
            "name": "Curation Statistics",
            "description": "Endpoints for monitoring analysis performance and curation readiness metrics."
        },
        {
            "name": "Cache Management",
            "description": "Endpoints for managing cached analysis results and improving performance."
        },
        {
            "name": "System",
            "description": "System health and status endpoints."
        }
    ]
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize components
text_processor = AdvancedTextProcessor()
model = None
if GEMINI_API_KEY:
    print("Model Status: Using configured model API")
else:
    print("Model Status: No model API key found. Model-based features will be limited.")
retriever = PubMedRetriever(api_key=NCBI_API_KEY)

qa_system = UnifiedQA(
    use_gemini=bool(GEMINI_API_KEY),
    gemini_api_key=GEMINI_API_KEY
)

# Initialize cache manager
cache_manager = CacheManager()

# Mount static files after API routes
static_dir = Path(__file__).parent.parent.parent / "frontend"
app.mount("/static", StaticFiles(directory=str(static_dir)), name="static")

class Message(BaseModel):
    content: str
    role: str = "user"
    model: Optional[str] = None

class Question(BaseModel):
    question: str

async def process_message(message):
    content = message.get('content', '')
    current_paper = message.get('currentPaper')
    if not content:
        return {"error": "No message content provided"}

    # If the user is discussing a paper, include its context
    if current_paper:
        metadata = retriever.get_paper_metadata(current_paper)
        context = f"Title: {metadata['title']}\nAbstract: {metadata['abstract']}\n"
        prompt = f"{context}\nUser question: {content}"
    else:
        # Otherwise, treat as a general chat
        prompt = content

    # Call Gemini chat with the prompt
    response = await qa_system.chat(prompt)
    return {
        "response": response["text"],
        "confidence": response.get("confidence"),
        # ... other fields as needed
    }

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """
    **WebSocket endpoint for real-time communication.**
    
    This endpoint handles WebSocket connections for:
    - Real-time paper analysis
    - Chat functionality
    - Live updates
    
    **Note:** This is a WebSocket endpoint and cannot be tested in the Swagger UI.
    Use a WebSocket client to connect to `/ws`.
    """
    await websocket.accept()
    print("WebSocket connection accepted")
    try:
        while True:
            data = await websocket.receive_text()
            try:
                message = json.loads(data)
                if message.get('type') == 'analyze_paper':
                    # Handle paper analysis request
                    pmid = message.get('pmid')
                    if not pmid:
                        await websocket.send_json({"error": "No PMID provided"})
                        continue
                    
                    try:
                        # Get paper metadata
                        metadata = retriever.get_paper_metadata(pmid)
                        if not metadata:
                            await websocket.send_json({"error": f"Paper with PMID {pmid} not found"})
                            continue
                        
                        # Get full text if available
                        full_text = ""
                        try:
                            full_text = retriever.get_pmc_fulltext(pmid)
                        except Exception as e:
                            print(f"Warning: Could not retrieve full text for PMID {pmid}: {str(e)}")
                        
                        # Analyze paper using Gemini
                        if GEMINI_API_KEY:
                            response = await qa_system.analyze_paper(
                                {"title": metadata["title"], "abstract": metadata["abstract"], "full_text": full_text}
                            )
                            
                            analysis_text = response.get("key_findings", [])
                            analysis_result = {
                                "type": "analysis_result",
                                "title": metadata["title"],
                                "authors": metadata.get("authors", "N/A"),
                                "journal": metadata.get("journal", "N/A"),
                                "date": metadata.get("publication_date", "N/A"),
                                "doi": metadata.get("doi", "N/A"),
                                "abstract": metadata["abstract"],
                                "key_findings": analysis_text,
                                "confidence": 0.8,
                                "status": "success",
                                "suggested_topics": [],
                                "found_terms": {},
                                "category_scores": {},
                                "num_tokens": len(analysis_text)
                            }
                            print("Sending analysis result to frontend:", analysis_result)
                            await websocket.send_json(analysis_result)
                        else:
                            await websocket.send_json({"error": "No model APIs available for analysis"})
                    except Exception as e:
                        print(f"Error analyzing paper: {str(e)}")
                        await websocket.send_json({"error": f"Error analyzing paper: {str(e)}"})
                else:
                    # Handle chat messages
                    user_message = message.get('content', '')
                    paper_ctx = message.get('paperContext')
                    chat_history = message.get('chatHistory', [])
                    if paper_ctx and paper_ctx.get('pmid'):
                        # Prepend paper metadata to prompt
                        context = f"You are discussing the following paper:\nTitle: {paper_ctx.get('title','')}\nAuthors: {paper_ctx.get('authors','')}\nJournal: {paper_ctx.get('journal','')}\nYear: {paper_ctx.get('year','')}\nPMID: {paper_ctx.get('pmid','')}\nAbstract: {paper_ctx.get('abstract','')}\n\nUser question: {user_message}"
                    elif chat_history:
                        # Build conversation context
                        history_str = ''
                        for msg in chat_history:
                            if msg.get('role') == 'user':
                                history_str += f"User: {msg.get('content','')}\n"
                            elif msg.get('role') == 'assistant':
                                history_str += f"Assistant: {msg.get('content','')}\n"
                        context = history_str + f"User: {user_message}"
                    else:
                        context = user_message
                    response = await qa_system.chat(context)
                    print("Sending chat response to frontend:", response)
                    await websocket.send_json({
                        "response": response["text"],
                        "confidence": response.get("confidence")
                    })
            except json.JSONDecodeError:
                await websocket.send_json({"error": "Invalid JSON format"})
            except Exception as e:
                print(f"Error processing message: {str(e)}")
                await websocket.send_json({"error": str(e)})
    except WebSocketDisconnect:
        print("WebSocket disconnected")
    except Exception as e:
        print(f"WebSocket error: {str(e)}")
        try:
            await websocket.send_json({"error": str(e)})
        except:
            pass

@app.post("/ask_question/{pmid}", tags=["Paper Analysis"])
async def ask_question(pmid: str, question: Question):
    """
    **Answer questions about a specific paper using AI analysis.**
    
    This endpoint allows users to ask specific questions about a paper and get AI-generated answers.
    Useful for getting clarification on specific aspects of a paper's content.
    
    **Parameters:**
    - `pmid`: PubMed ID of the paper
    - `question`: The question object containing the user's query
    
    **Returns:**
    - **answer**: AI-generated response to the question
    - **confidence**: Confidence score for the answer
    
    **Note:** This endpoint uses the paper's metadata and abstract for context.
    """
    try:
        # Get paper metadata
        metadata = retriever.get_paper_metadata(pmid)
        if not metadata:
            raise HTTPException(status_code=404, detail="Paper not found")
        
        # Create context from paper metadata
        context = f"Title: {metadata['title']}\nAbstract: {metadata['abstract']}"
        
        # Try to get full text if available
        try:
            full_text = retriever.get_pmc_fulltext(pmid)
            if full_text:
                # Limit full text to avoid token limits
                context += f"\n\nFull Text (excerpt): {full_text[:2000]}..."
        except Exception as e:
            print(f"Error getting full text for PMID {pmid}: {str(e)}")
            # Continue with just the abstract
        
        # Try models in order of preference
        models_to_try = ["gemini"] if "gemini" in AVAILABLE_MODELS else AVAILABLE_MODELS
        answer = None
        confidence = 0.0
        
        for model_name in models_to_try:
            try:
                if model_name == "gemini" and GEMINI_API_KEY:
                    print(f"Attempting to use Gemini for question answering")
                    response = await qa_system.analyze_paper(
                        {"title": context, "abstract": question.question, "full_text": ""}
                    )
                    answer = response.get("key_findings", [])
                    confidence = 0.8  # Gemini responses are generally reliable
                    break
                    
            except Exception as e:
                print(f"Error with {model_name}: {str(e)}")
                continue
        
        if answer is None:
            return {
                "answer": "I apologize, but I'm currently unable to process your question. All available AI models are experiencing issues. Please try again later.",
                "confidence": 0.0
            }
        
        return {
            "answer": answer,
            "confidence": confidence
        }
        
    except HTTPException as he:
        raise he
    except Exception as e:
        print(f"Error in ask_question endpoint: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error answering question: {str(e)}")

@app.post("/upload_csv", tags=["Batch Processing"])
async def upload_csv(file: UploadFile = File(...)):
    """
    **Upload CSV file with PMIDs for batch processing.**
    
    This endpoint processes a CSV file containing PubMed IDs and analyzes each paper
    for BugSigDB curation readiness using the 6 essential fields.
    
    **Parameters:**
    - `file`: CSV file with PMIDs (first column should contain PMIDs)
    
    **CSV Format:**
    - First column: PubMed IDs (numeric)
    - Additional columns: Optional metadata (not required)
    
    **Returns:**
    - **results**: List of analysis results for each PMID
    - **total_processed**: Number of PMIDs successfully processed
    
    **Limitations:**
    - Maximum 10 PMIDs processed per upload (for performance)
    - Only CSV files supported
    - PMIDs must be numeric
    
    **Analysis Results:**
    Each result contains the 6 essential fields analysis with curation readiness status.
    """
    try:
        # Check file type
        if not file.filename.endswith('.csv'):
            raise HTTPException(status_code=400, detail="Only CSV files are supported")
        
        # Read file content
        content = await file.read()
        csv_text = content.decode("utf-8")
        
        # Parse CSV to extract PMIDs (assuming first column contains PMIDs)
        import csv
        from io import StringIO
        
        csv_reader = csv.reader(StringIO(csv_text))
        pmids = []
        
        for row in csv_reader:
            if row and row[0].strip().isdigit():  # Check if first column is a numeric PMID
                pmids.append(row[0].strip())
        
        if not pmids:
            raise HTTPException(status_code=400, detail="No valid PMIDs found in CSV file")
        
        # Process each PMID using the enhanced analysis
        results = []
        for pmid in pmids[:10]:  # Limit to first 10 PMIDs for performance
            try:
                # Get paper metadata
                metadata = retriever.get_paper_metadata(pmid)
                if not metadata:
                    results.append({
                        "pmid": pmid,
                        "title": "Not found",
                        "authors": "N/A",
                        "journal": "N/A",
                        "date": "N/A",
                        "enhanced_analysis": {
                            "host_species": {"status": "ABSENT", "reason": "Paper not found", "suggestion": "Verify PMID"},
                            "body_site": {"status": "ABSENT", "reason": "Paper not found", "suggestion": "Verify PMID"},
                            "condition": {"status": "ABSENT", "reason": "Paper not found", "suggestion": "Verify PMID"},
                            "sequencing_type": {"status": "ABSENT", "reason": "Paper not found", "suggestion": "Verify PMID"},
                            "taxa_level": {"status": "ABSENT", "reason": "Paper not found", "suggestion": "Verify PMID"},
                            "sample_size": {"status": "ABSENT", "reason": "Paper not found", "suggestion": "Verify PMID"}
                        }
                    })
                    continue
                
                # Get full text if available
                full_text = ""
                try:
                    full_text = retriever.get_pmc_fulltext(pmid)
                    if isinstance(full_text, str):
                        try:
                            soup = BeautifulSoup(full_text, 'lxml')
                            full_text = retriever._extract_text_from_pmc_xml(soup)
                        except Exception as e:
                            print(f"Failed to parse PMC XML for PMID {pmid}: {str(e)}")
                    elif isinstance(full_text, list):
                        full_text = '\n'.join(str(x) for x in full_text)
                    elif full_text is None:
                        full_text = ""
                    else:
                        full_text = str(full_text)
                except Exception as e:
                    print(f"Failed to retrieve PMC full text for PMID {pmid}: {str(e)}")
                    full_text = ""
                
                # Analyze paper using enhanced analysis with the 6 essential fields
                enhanced_prompt = f"""
                Analyze this scientific paper for BugSigDB curation. Focus ONLY on these 6 essential fields:

                PAPER INFORMATION:
                Title: {metadata.get('title', '')}
                Abstract: {metadata.get('abstract', '')}
                Full Text: {full_text[:3000] if full_text else 'Not available'}

                REQUIRED ANALYSIS - ONLY THESE 6 FIELDS:
                1. HOST SPECIES: What is the host species? (e.g., Human, Mouse, Rat, etc.)
                2. BODY SITE: What is the body site habitat of the microbiome? (e.g., Gut, Oral, Skin, etc.)
                3. CONDITION: What condition, treatment, or exposure is being studied?
                4. SEQUENCING TYPE: What molecular method was used? (e.g., 16S, metagenomics, etc.)
                5. TAXA LEVEL: What taxonomic level was analyzed? (e.g., phylum, genus, species, etc.)
                6. SAMPLE SIZE: What is the number of samples analyzed?

                IMPORTANT ANALYSIS REQUIREMENTS:
                - For each field, determine if the information is PRESENT, PARTIALLY_PRESENT, or ABSENT
                - If PRESENT: Extract the specific value and provide high confidence (0.8-1.0)
                - If PARTIALLY_PRESENT: Extract what's available and provide medium confidence (0.4-0.7)
                - If ABSENT: Provide reason why it's missing and confidence 0.0
                - For missing fields, suggest what additional information would be needed for curation

                Please provide your analysis in this exact JSON format:
                {{
                    "host_species": {{
                        "primary": "species_name",
                        "confidence": 0.0-1.0,
                        "status": "PRESENT|PARTIALLY_PRESENT|ABSENT",
                        "reason_if_missing": "explanation if absent",
                        "suggestions_for_curation": "what additional info is needed"
                    }},
                    "body_site": {{
                        "site": "site_name",
                        "confidence": 0.0-1.0,
                        "status": "PRESENT|PARTIALLY_PRESENT|ABSENT",
                        "reason_if_missing": "explanation if absent",
                        "suggestions_for_curation": "what additional info is needed"
                    }},
                    "condition": {{
                        "description": "detailed_description",
                        "confidence": 0.0-1.0,
                        "status": "PRESENT|PARTIALLY_PRESENT|ABSENT",
                        "reason_if_missing": "explanation if absent",
                        "suggestions_for_curation": "what additional info is needed"
                    }},
                    "sequencing_type": {{
                        "method": "method_name",
                        "confidence": 0.0-1.0,
                        "status": "PRESENT|PARTIALLY_PRESENT|ABSENT",
                        "reason_if_missing": "explanation if absent",
                        "suggestions_for_curation": "what additional info is needed"
                    }},
                    "taxa_level": {{
                        "level": "taxonomic_level",
                        "confidence": 0.0-1.0,
                        "status": "PRESENT|PARTIALLY_PRESENT|ABSENT",
                        "reason_if_missing": "explanation if absent",
                        "suggestions_for_curation": "what additional info is needed"
                    }},
                    "sample_size": {{
                        "size": "sample_count",
                        "confidence": 0.0-1.0,
                        "status": "PRESENT|PARTIALLY_PRESENT|ABSENT",
                        "reason_if_missing": "explanation if absent",
                        "suggestions_for_curation": "what additional info is needed"
                    }},
                    "curation_ready": true/false,
                    "missing_fields": ["field1", "field2", ...],
                    "curation_preparation_summary": "Overall assessment of what's needed for curation"
                }}

                CRITICAL INSTRUCTIONS: 
                - Focus ONLY on the 6 fields listed above
                - Do NOT include Factor-Based Analysis, Detailed Explanation, Specific Reasons, Examples and Evidence, Key Findings, Category Scores, or Analysis Confidence
                - For each missing field, provide specific reason and suggestions
                - Determine curation readiness based on having all 6 fields with status "PRESENT"
                - Return ONLY the JSON structure above
                """
                
                analysis = await qa_system.analyze_paper_enhanced(enhanced_prompt)
                
                # Parse the JSON response from Gemini
                try:
                    parsed_analysis = json.loads(analysis.get("key_findings", "{}"))
                    
                    # Validate that we have exactly the 6 required fields
                    required_fields = ["host_species", "body_site", "condition", "sequencing_type", "taxa_level", "sample_size"]
                    missing_fields = []
                    
                    for field in required_fields:
                        if field not in parsed_analysis or not parsed_analysis[field]:
                            missing_fields.append(field)
                            # Ensure the field exists with default structure
                            if field == "host_species":
                                parsed_analysis[field] = {"primary": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "Field not found in analysis", "suggestions_for_curation": "Review paper for host species information"}
                            elif field == "body_site":
                                parsed_analysis[field] = {"site": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "Field not found in analysis", "suggestions_for_curation": "Review paper for body site information"}
                            elif field == "condition":
                                parsed_analysis[field] = {"description": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "Field not found in analysis", "suggestions_for_curation": "Review paper for condition information"}
                            elif field == "sequencing_type":
                                parsed_analysis[field] = {"method": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "Field not found in analysis", "suggestions_for_curation": "Review paper for sequencing method information"}
                            elif field == "taxa_level":
                                parsed_analysis[field] = {"level": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "Field not found in analysis", "suggestions_for_curation": "Review paper for taxonomic level information"}
                            elif field == "sample_size":
                                parsed_analysis[field] = {"size": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "Field not found in analysis", "suggestions_for_curation": "Review paper for sample size information"}
                    
                    # Determine curation readiness based on having all 6 fields with status "PRESENT"
                    curation_ready = len(missing_fields) == 0 and all(
                        parsed_analysis.get(field, {}).get("status") == "PRESENT" 
                        for field in required_fields
                    )
                    
                    # Update the parsed analysis with curation readiness
                    parsed_analysis["curation_ready"] = curation_ready
                    parsed_analysis["missing_fields"] = missing_fields
                    
                except json.JSONDecodeError:
                    # Fallback if JSON parsing fails - ensure we have exactly the 6 fields
                    parsed_analysis = {
                        "host_species": {"primary": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "JSON parsing failed", "suggestions_for_curation": "Re-run analysis"},
                        "body_site": {"site": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "JSON parsing failed", "suggestions_for_curation": "Re-run analysis"},
                        "condition": {"description": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "JSON parsing failed", "suggestions_for_curation": "Re-run analysis"},
                        "sequencing_type": {"method": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "JSON parsing failed", "suggestions_for_curation": "Re-run analysis"},
                        "taxa_level": {"level": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "JSON parsing failed", "suggestions_for_curation": "Re-run analysis"},
                        "sample_size": {"size": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "JSON parsing failed", "suggestions_for_curation": "Re-run analysis"},
                        "curation_ready": False,
                        "missing_fields": ["host_species", "body_site", "condition", "sequencing_type", "taxa_level", "sample_size"],
                        "curation_preparation_summary": "Analysis failed - re-run required"
                    }
                
                # Store analysis results in cache
                cache_manager.store_analysis_result(
                    pmid, 
                    parsed_analysis, 
                    metadata, 
                    "gemini_enhanced", 
                    analysis.get("confidence", 0.0), 
                    curation_ready
                )
                
                results.append({
                    "pmid": pmid,
                    "metadata": metadata,
                    "enhanced_analysis": parsed_analysis,
                    "curation_ready": curation_ready
                })
                
            except Exception as e:
                print(f"Error processing PMID {pmid}: {str(e)}")
                results.append({
                    "pmid": pmid,
                    "title": "Error processing",
                    "authors": "N/A",
                    "journal": "N/A",
                    "date": "N/A",
                    "enhanced_analysis": {
                        "host_species": {"status": "ABSENT", "reason": f"Processing error: {str(e)}", "suggestion": "Try again later"},
                        "body_site": {"status": "ABSENT", "reason": f"Processing error: {str(e)}", "suggestion": "Try again later"},
                        "condition": {"status": "ABSENT", "reason": f"Processing error: {str(e)}", "suggestion": "Try again later"},
                        "sequencing_type": {"status": "ABSENT", "reason": f"Processing error: {str(e)}", "suggestion": "Try again later"},
                        "taxa_level": {"status": "ABSENT", "reason": f"Processing error: {str(e)}", "suggestion": "Try again later"},
                        "sample_size": {"status": "ABSENT", "reason": f"Processing error: {str(e)}", "suggestion": "Try again later"}
                    }
                })
        
        return {"results": results, "total_processed": len(results)}
        
    except HTTPException as he:
        raise he
    except Exception as e:
        print(f"Error processing CSV file: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error processing CSV file: {str(e)}")

@app.post("/submit_curation", tags=["Curation Statistics"])
async def submit_curation(request: Request):
    """
    **Submit paper curation to BugSigDB.**
    
    This endpoint allows users to submit curation data for papers that have been analyzed.
    Currently logs the submission (placeholder for future BugSigDB API integration).
    
    **Required Fields:**
    - **pmid**: PubMed ID of the paper
    - **title**: Paper title
    - **host**: Host species
    - **body_site**: Body site location
    - **condition**: Condition studied
    - **sequencing_type**: Sequencing method used
    
    **Returns:**
    - **status**: Success/failure status
    - **message**: Confirmation message
    
    **Note:** This is a placeholder endpoint for future BugSigDB integration.
    """
    try:
        # Parse request body
        data = await request.json()
        
        # Validate required fields
        required_fields = ["pmid", "title", "host", "body_site", "condition", "sequencing_type"]
        missing_fields = [field for field in required_fields if not data.get(field)]
        if missing_fields:
            raise HTTPException(status_code=400, detail=f"Missing required fields: {', '.join(missing_fields)}")
        
        # In a real implementation, this would submit to BugSigDB API
        # For now, just log the submission
        print(f"Curation submitted: {data}")
        
        # Return success response
        return {"status": "success", "message": "Curation submitted successfully"}
    except HTTPException as he:
        raise he
    except Exception as e:
        print(f"Error submitting curation: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error submitting curation: {str(e)}")

def extract_taxa(text):
    """Extract potential taxa from text"""
    # This is a simplified implementation
    # In a real system, this would use a more sophisticated approach
    
    # Common bacterial genera
    common_genera = [
        "Bacteroides", "Prevotella", "Faecalibacterium", "Bifidobacterium",
        "Lactobacillus", "Escherichia", "Streptococcus", "Staphylococcus",
        "Clostridium", "Ruminococcus", "Akkermansia", "Pseudomonas"
    ]
    
    found_taxa = []
    for genus in common_genera:
        if re.search(r'\b' + genus + r'\b', text, re.IGNORECASE):
            found_taxa.append(genus)
    
    return found_taxa

@app.get("/model_status", tags=["System"])
async def get_model_status():
    """
    **Get the status of available AI models.**
    
    This endpoint provides information about which AI models are available
    and their current status for analysis operations.
    
    **Returns:**
    - **available_models**: List of available model names
    - **default_model**: Currently selected default model
    - **gemini_available**: Whether Gemini model is available
    - **gemini_key_present**: Whether Gemini API key is configured
    """
    try:
        status = {
            "available_models": AVAILABLE_MODELS,
            "default_model": DEFAULT_MODEL,
            "gemini_available": bool(GEMINI_API_KEY),
            "gemini_key_present": bool(GEMINI_API_KEY)
        }
        return status
    except Exception as e:
        print(f"Error getting model status: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error getting model status: {str(e)}")

@app.get("/analyze_by_title", tags=["Paper Analysis"])
async def analyze_by_title(title: str, request: Request):
    """
    **Analyze a paper by its title.**
    
    This endpoint searches for a paper by title and then analyzes it
    for BugSigDB curation readiness using the 6 essential fields.
    
    **Parameters:**
    - `title`: Title of the paper to search for
    
    **Returns:**
    - Same response format as `/analyze/{pmid}` endpoint
    
    **Note:** This endpoint first searches PubMed for the title, then performs the analysis.
    """
    try:
        # Search PubMed for the paper by title
        search_results = retriever.search_by_title(title)
        if not search_results:
            raise HTTPException(status_code=404, detail=f"No paper found with title: {title}")
        
        # Get the first result's PMID
        pmid = search_results[0].get('pmid')
        if not pmid:
            raise HTTPException(status_code=404, detail="No PMID found for the paper")
        
        # Use the existing analyze_paper endpoint
        return await analyze_paper(pmid, request)
    except HTTPException as he:
        raise he
    except Exception as e:
        print(f"Error analyzing paper by title: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error analyzing paper: {str(e)}")

@app.get("/analyze_by_url", tags=["Paper Analysis"])
async def analyze_by_url(url: str, request: Request):
    """
    **Analyze a paper by its URL (DOI or PubMed URL).**
    
    This endpoint extracts DOI or PMID from a URL and then analyzes the paper
    for BugSigDB curation readiness using the 6 essential fields.
    
    **Parameters:**
    - `url`: URL containing DOI or PubMed link
    
    **Supported URL formats:**
    - DOI URLs: `https://doi.org/10.1234/example`
    - PubMed URLs: `https://pubmed.ncbi.nlm.nih.gov/12345678/`
    
    **Returns:**
    - Same response format as `/analyze/{pmid}` endpoint
    
    **Note:** This endpoint automatically detects URL type and routes to appropriate analysis.
    """
    try:
        # Extract DOI or PMID from URL
        doi = None
        pmid = None
        
        # Check if it's a DOI URL
        if "doi.org" in url:
            doi = url.split("doi.org/")[-1]
        # Check if it's a PubMed URL
        elif "pubmed.ncbi.nlm.nih.gov" in url:
            pmid = url.split("/")[-1]
        else:
            raise HTTPException(status_code=400, detail="Invalid URL format. Please provide a DOI or PubMed URL")
        
        if doi:
            # Use the existing fetch_by_doi endpoint
            return await fetch_by_doi(doi, request)
        elif pmid:
            # Use the existing analyze_paper endpoint
            return await analyze_paper(pmid, request)
    except HTTPException as he:
        raise he
    except Exception as e:
        print(f"Error analyzing paper by URL: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Error analyzing paper: {str(e)}")

@app.get("/", tags=["System"])
async def root():
    """Redirect to the frontend application."""
    return RedirectResponse(url="/static/index.html")

@app.get("/analyze/{pmid}", tags=["Paper Analysis"])
async def analyze_paper(pmid: str):
    """
    **Analyze a single paper for BugSigDB curation readiness.**
    
    This endpoint analyzes a scientific paper using AI to determine if it's ready for BugSigDB curation.
    It focuses on extracting and validating 6 essential fields required for curation.
    
    **Parameters:**
    - `pmid`: PubMed ID of the paper to analyze
    
    **Returns:**
    - **enhanced_analysis**: Detailed analysis of the 6 essential fields
    - **curation_ready**: Boolean indicating if the paper is ready for curation
    - **metadata**: Paper metadata (title, abstract, authors, etc.)
    
    **6 Essential Fields Analyzed:**
    1. **host_species**: Host organism being studied
    2. **body_site**: Microbiome sample collection location
    3. **condition**: Disease/treatment/exposure studied
    4. **sequencing_type**: Molecular method used
    5. **taxa_level**: Taxonomic level analyzed
    6. **sample_size**: Number of samples analyzed
    
    **Field Status Values:**
    - **PRESENT**: Information is complete and clear
    - **PARTIALLY_PRESENT**: Some information available but incomplete
    - **ABSENT**: Information is missing with reasons and suggestions
    
    **Curation Readiness:**
    A paper is considered ready for curation when ALL 6 fields have status "PRESENT".
    """
    try:
        logger.info(f"=== Starting analysis for PMID: {pmid} ===")
        
        # Check cache first for analysis results
        cached_result = cache_manager.get_analysis_result(pmid)
        if cached_result and cache_manager.is_cache_valid(cached_result["timestamp"]):
            logger.info(f"Returning cached analysis for PMID: {pmid}")
            return {
                "pmid": pmid,
                "metadata": cached_result["metadata"],
                "enhanced_analysis": cached_result["analysis_data"],
                "curation_ready": cached_result["curation_ready"],
                "timestamp": cached_result["timestamp"],
                "source": cached_result["source"],
                "cached": True
            }
        
        # Get paper metadata
        metadata = retriever.get_paper_metadata(pmid)
        csv_metadata = get_paper_metadata_from_csv(pmid)
        
        if csv_metadata:
            metadata.update(csv_metadata)
        
        if not metadata:
            raise HTTPException(status_code=404, detail=f"Paper not found: {pmid}")
        
        # Store metadata in cache
        cache_manager.store_metadata(pmid, metadata, "pubmed")
        
        # Get full text if available
        full_text = ""
        try:
            full_text = retriever.get_pmc_fulltext(pmid)
            if isinstance(full_text, str):
                try:
                    soup = BeautifulSoup(full_text, 'lxml')
                    full_text = retriever._extract_text_from_pmc_xml(soup)
                except Exception as e:
                    logger.warning(f"Failed to parse PMC XML for PMID {pmid}: {str(e)}")
            
            # Store full text in cache
            if full_text:
                cache_manager.store_fulltext(pmid, full_text, "pmc")
                
        except Exception as e:
            logger.warning(f"Failed to retrieve PMC full text for PMID {pmid}: {str(e)}")
            full_text = ""
        
        # Create enhanced prompt for specific analysis - same as enhanced endpoints
        enhanced_prompt = f"""
        Analyze this scientific paper for BugSigDB curation. Focus ONLY on these 6 essential fields:

        PAPER INFORMATION:
        Title: {metadata.get('title', '')}
        Abstract: {metadata.get('abstract', '')}
        Full Text: {full_text[:3000] if full_text else 'Not available'}

        REQUIRED ANALYSIS - ONLY THESE 6 FIELDS:
        1. HOST SPECIES: What is the host species? (e.g., Human, Mouse, Rat, etc.)
        2. BODY SITE: What is the body site habitat of the microbiome? (e.g., Gut, Oral, Skin, etc.)
        3. CONDITION: What condition, treatment, or exposure is being studied?
        4. SEQUENCING TYPE: What molecular method was used? (e.g., 16S, metagenomics, etc.)
        5. TAXA LEVEL: What taxonomic level was analyzed? (e.g., phylum, genus, species, etc.)
        6. SAMPLE SIZE: What is the number of samples analyzed?

        IMPORTANT ANALYSIS REQUIREMENTS:
        - For each field, determine if the information is PRESENT, PARTIALLY_PRESENT, or ABSENT
        - If PRESENT: Extract the specific value and provide high confidence (0.8-1.0)
        - If PARTIALLY_PRESENT: Extract what's available and provide medium confidence (0.4-0.7)
        - If ABSENT: Provide reason why it's missing and confidence 0.0
        - For missing fields, suggest what additional information would be needed for curation

        Please provide your analysis in this exact JSON format:
        {{
            "host_species": {{
                "primary": "species_name",
                "confidence": 0.0-1.0,
                "status": "PRESENT|PARTIALLY_PRESENT|ABSENT",
                "reason_if_missing": "explanation if absent",
                "suggestions_for_curation": "what additional info is needed"
            }},
            "body_site": {{
                "site": "site_name",
                "confidence": 0.0-1.0,
                "status": "PRESENT|PARTIALLY_PRESENT|ABSENT",
                "reason_if_missing": "explanation if absent",
                "suggestions_for_curation": "what additional info is needed"
            }},
            "condition": {{
                "description": "detailed_description",
                "confidence": 0.0-1.0,
                "status": "PRESENT|PARTIALLY_PRESENT|ABSENT",
                "reason_if_missing": "explanation if absent",
                "suggestions_for_curation": "what additional info is needed"
            }},
            "sequencing_type": {{
                "method": "method_name",
                "confidence": 0.0-1.0,
                "status": "PRESENT|PARTIALLY_PRESENT|ABSENT",
                "reason_if_missing": "explanation if absent",
                "suggestions_for_curation": "what additional info is needed"
            }},
            "taxa_level": {{
                "level": "taxonomic_level",
                "confidence": 0.0-1.0,
                "status": "PRESENT|PARTIALLY_PRESENT|ABSENT",
                "reason_if_missing": "explanation if absent",
                "suggestions_for_curation": "what additional info is needed"
            }},
            "sample_size": {{
                "size": "sample_count",
                "confidence": 0.0-1.0,
                "status": "PRESENT|PARTIALLY_PRESENT|ABSENT",
                "reason_if_missing": "explanation if absent",
                "suggestions_for_curation": "what additional info is needed"
            }},
            "curation_ready": true/false,
            "missing_fields": ["field1", "field2", ...],
            "curation_preparation_summary": "Overall assessment of what's needed for curation"
        }}

        CRITICAL INSTRUCTIONS: 
        - Focus ONLY on the 6 fields listed above
        - Do NOT include Factor-Based Analysis, Detailed Explanation, Specific Reasons, Examples and Evidence, Key Findings, Category Scores, or Analysis Confidence
        - For each missing field, provide specific reason and suggestions
        - Determine curation readiness based on having all 6 fields with status "PRESENT"
        - Return ONLY the JSON structure above
        """
        
        # Run enhanced analysis using Gemini
        try:
            analysis = await qa_system.analyze_paper_enhanced(enhanced_prompt)
            
            # Parse the JSON response from Gemini
            try:
                parsed_analysis = json.loads(analysis.get("key_findings", "{}"))
                
                # Validate that we have exactly the 6 required fields
                required_fields = ["host_species", "body_site", "condition", "sequencing_type", "taxa_level", "sample_size"]
                missing_fields = []
                
                for field in required_fields:
                    if field not in parsed_analysis or not parsed_analysis[field]:
                        missing_fields.append(field)
                        # Ensure the field exists with default structure
                        if field == "host_species":
                            parsed_analysis[field] = {"primary": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "Field not found in analysis", "suggestions_for_curation": "Review paper for host species information"}
                        elif field == "body_site":
                            parsed_analysis[field] = {"site": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "Field not found in analysis", "suggestions_for_curation": "Review paper for body site information"}
                        elif field == "condition":
                            parsed_analysis[field] = {"description": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "Field not found in analysis", "suggestions_for_curation": "Review paper for condition information"}
                        elif field == "sequencing_type":
                            parsed_analysis[field] = {"method": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "Field not found in analysis", "suggestions_for_curation": "Review paper for sequencing method information"}
                        elif field == "taxa_level":
                            parsed_analysis[field] = {"level": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "Field not found in analysis", "suggestions_for_curation": "Review paper for taxonomic level information"}
                        elif field == "sample_size":
                            parsed_analysis[field] = {"size": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "Field not found in analysis", "suggestions_for_curation": "Review paper for sample size information"}
                
                # Determine curation readiness based on having all 6 fields with status "PRESENT"
                curation_ready = len(missing_fields) == 0 and all(
                    parsed_analysis.get(field, {}).get("status") == "PRESENT" 
                    for field in required_fields
                )
                
                # Update the parsed analysis with curation readiness
                parsed_analysis["curation_ready"] = curation_ready
                parsed_analysis["missing_fields"] = missing_fields
                
            except json.JSONDecodeError:
                # Fallback if JSON parsing fails - ensure we have exactly the 6 fields
                parsed_analysis = {
                    "host_species": {"primary": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "JSON parsing failed", "suggestions_for_curation": "Re-run analysis"},
                    "body_site": {"site": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "JSON parsing failed", "suggestions_for_curation": "Re-run analysis"},
                    "condition": {"description": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "JSON parsing failed", "suggestions_for_curation": "Re-run analysis"},
                    "sequencing_type": {"method": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "JSON parsing failed", "suggestions_for_curation": "Re-run analysis"},
                    "taxa_level": {"level": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "JSON parsing failed", "suggestions_for_curation": "Re-run analysis"},
                    "sample_size": {"size": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "JSON parsing failed", "suggestions_for_curation": "Re-run analysis"},
                    "curation_ready": False,
                    "missing_fields": ["host_species", "body_site", "condition", "sequencing_type", "taxa_level", "sample_size"],
                    "curation_preparation_summary": "Analysis failed - re-run required"
                }
            
            # Store analysis results in cache
            cache_manager.store_analysis_result(
                pmid, 
                parsed_analysis, 
                metadata, 
                "gemini_enhanced", 
                analysis.get("confidence", 0.0), 
                curation_ready
            )
            
            # Compose the enhanced response
            response = {
                "pmid": pmid,
                "metadata": metadata,
                "enhanced_analysis": parsed_analysis,
                "curation_ready": curation_ready,
                "timestamp": datetime.now().isoformat(),
                "source": "gemini_enhanced_analysis",
                "cached": False
            }
            
            return response
            
        except Exception as e:
            logger.error(f"Error during enhanced analysis: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")
            
    except HTTPException as he:
        raise he
    except Exception as e:
        logger.error(f"Error in analyze_paper endpoint: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Unexpected error: {str(e)}")

@app.post("/analyze_batch", tags=["Batch Processing"])
async def analyze_batch(pmids: list = Body(...), page: int = Query(1), page_size: int = Query(20)):
    """
    **Batch analysis endpoint for multiple papers.**
    
    This endpoint analyzes multiple papers at once for BugSigDB curation readiness.
    It processes papers in batches and returns analysis results for the 6 essential fields.
    
    **Parameters:**
    - `pmids`: List of PubMed IDs to analyze
    - `page`: Page number for pagination (default: 1)
    - `page_size`: Number of papers per page (default: 20)
    
    **Returns:**
    - List of analysis results, each containing:
        - **enhanced_analysis**: 6-field analysis results
        - **curation_ready**: Boolean indicating curation readiness
        - **metadata**: Paper metadata
        - **status**: Success/error status
    
    **Performance:** Results are cached to avoid re-analysis of previously processed papers.
    """
    start = (page - 1) * page_size
    end = start + page_size
    pmids_batch = pmids[start:end]
    results = []
    
    for pmid in pmids_batch:
        try:
            # Check cache first for analysis results
            cached_result = cache_manager.get_analysis_result(pmid)
            if cached_result and cache_manager.is_cache_valid(cached_result["timestamp"]):
                results.append({
                    "pmid": pmid,
                    "metadata": cached_result["metadata"],
                    "enhanced_analysis": cached_result["analysis_data"],
                    "curation_ready": cached_result["curation_ready"],
                    "timestamp": cached_result["timestamp"],
                    "source": cached_result["source"],
                    "cached": True,
                    "status": "success"
                })
                continue
            
            # Get metadata
            metadata = retriever.get_paper_metadata(pmid)
            csv_metadata = get_paper_metadata_from_csv(pmid)
            
            if csv_metadata:
                metadata.update(csv_metadata)
            
            if not metadata:
                results.append({
                    "pmid": pmid,
                    "status": "error",
                    "error": "Paper not found"
                })
                continue
            
            # Store metadata in cache
            cache_manager.store_metadata(pmid, metadata, "pubmed")
            
            # Get full text if available
            full_text = ""
            try:
                full_text = retriever.get_pmc_fulltext(pmid)
                if isinstance(full_text, str):
                    try:
                        soup = BeautifulSoup(full_text, 'lxml')
                        full_text = retriever._extract_text_from_pmc_xml(soup)
                    except Exception as e:
                        logger.warning(f"Failed to parse PMC XML for PMID {pmid}: {str(e)}")
                
                if full_text:
                    cache_manager.store_fulltext(pmid, full_text, "pmc")
            except Exception as e:
                logger.warning(f"Failed to retrieve PMC full text for PMID {pmid}: {str(e)}")
            
            # Run enhanced analysis with the same improved prompt
            enhanced_prompt = f"""
            Analyze this scientific paper for BugSigDB curation. Focus ONLY on these 6 essential fields:

            PAPER INFORMATION:
            Title: {metadata.get('title', '')}
            Abstract: {metadata.get('abstract', '')}
            Full Text: {full_text[:3000] if full_text else 'Not available'}

            REQUIRED ANALYSIS - ONLY THESE 6 FIELDS:
            1. HOST SPECIES: What is the host species? (e.g., Human, Mouse, Rat, etc.)
            2. BODY SITE: What is the body site habitat of the microbiome? (e.g., Gut, Oral, Skin, etc.)
            3. CONDITION: What condition, treatment, or exposure is being studied?
            4. SEQUENCING TYPE: What molecular method was used? (e.g., 16S, metagenomics, etc.)
            5. TAXA LEVEL: What taxonomic level was analyzed? (e.g., phylum, genus, species, etc.)
            6. SAMPLE SIZE: What is the number of samples analyzed?

            IMPORTANT ANALYSIS REQUIREMENTS:
            - For each field, determine if the information is PRESENT, PARTIALLY_PRESENT, or ABSENT
            - If PRESENT: Extract the specific value and provide high confidence (0.8-1.0)
            - If PARTIALLY_PRESENT: Extract what's available and provide medium confidence (0.4-0.7)
            - If ABSENT: Provide reason why it's missing and confidence 0.0
            - For missing fields, suggest what additional information would be needed for curation

            Please provide your analysis in this exact JSON format:
            {{
                "host_species": {{
                    "primary": "species_name",
                    "confidence": 0.0-1.0,
                    "status": "PRESENT|PARTIALLY_PRESENT|ABSENT",
                    "reason_if_missing": "explanation if absent",
                    "suggestions_for_curation": "what additional info is needed"
                }},
                "body_site": {{
                    "site": "site_name",
                    "confidence": 0.0-1.0,
                    "status": "PRESENT|PARTIALLY_PRESENT|ABSENT",
                    "reason_if_missing": "explanation if absent",
                    "suggestions_for_curation": "what additional info is needed"
                }},
                "condition": {{
                    "description": "detailed_description",
                    "confidence": 0.0-1.0,
                    "status": "PRESENT|PARTIALLY_PRESENT|ABSENT",
                    "reason_if_missing": "explanation if absent",
                    "suggestions_for_curation": "what additional info is needed"
                }},
                "sequencing_type": {{
                    "method": "method_name",
                    "confidence": 0.0-1.0,
                    "status": "PRESENT|PARTIALLY_PRESENT|ABSENT",
                    "reason_if_missing": "explanation if absent",
                    "suggestions_for_curation": "what additional info is needed"
                }},
                "taxa_level": {{
                    "level": "taxonomic_level",
                    "confidence": 0.0-1.0,
                    "status": "PRESENT|PARTIALLY_PRESENT|ABSENT",
                    "reason_if_missing": "explanation if absent",
                    "suggestions_for_curation": "what additional info is needed"
                }},
                "sample_size": {{
                    "size": "sample_count",
                    "confidence": 0.0-1.0,
                    "status": "PRESENT|PARTIALLY_PRESENT|ABSENT",
                    "reason_if_missing": "explanation if absent",
                    "suggestions_for_curation": "what additional info is needed"
                }},
                "curation_ready": true/false,
                "missing_fields": ["field1", "field2", ...],
                "curation_preparation_summary": "Overall assessment of what's needed for curation"
            }}

            CRITICAL INSTRUCTIONS: 
            - Focus ONLY on the 6 fields listed above
            - Do NOT include Factor-Based Analysis, Detailed Explanation, Specific Reasons, Examples and Evidence, Key Findings, Category Scores, or Analysis Confidence
            - For each missing field, provide specific reason and suggestions
            - Determine curation readiness based on having all 6 fields with status "PRESENT"
            - Return ONLY the JSON structure above
            """
            
            analysis = await qa_system.analyze_paper_enhanced(enhanced_prompt)
            
            try:
                parsed_analysis = json.loads(analysis.get("key_findings", "{}"))
                
                # Validate that we have exactly the 6 required fields
                required_fields = ["host_species", "body_site", "condition", "sequencing_type", "taxa_level", "sample_size"]
                missing_fields = []
                
                for field in required_fields:
                    if field not in parsed_analysis or not parsed_analysis[field]:
                        missing_fields.append(field)
                        # Ensure the field exists with default structure
                        if field == "host_species":
                            parsed_analysis[field] = {"primary": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "Field not found in analysis", "suggestions_for_curation": "Review paper for host species information"}
                        elif field == "body_site":
                            parsed_analysis[field] = {"site": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "Field not found in analysis", "suggestions_for_curation": "Review paper for body site information"}
                        elif field == "condition":
                            parsed_analysis[field] = {"description": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "Field not found in analysis", "suggestions_for_curation": "Review paper for condition information"}
                        elif field == "sequencing_type":
                            parsed_analysis[field] = {"method": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "Field not found in analysis", "suggestions_for_curation": "Review paper for sequencing method information"}
                        elif field == "taxa_level":
                            parsed_analysis[field] = {"level": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "Field not found in analysis", "suggestions_for_curation": "Review paper for taxonomic level information"}
                        elif field == "sample_size":
                            parsed_analysis[field] = {"size": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "Field not found in analysis", "suggestions_for_curation": "Review paper for sample size information"}
                
                # Determine curation readiness based on having all 6 fields with status "PRESENT"
                curation_ready = len(missing_fields) == 0 and all(
                    parsed_analysis.get(field, {}).get("status") == "PRESENT" 
                    for field in required_fields
                )
                
                # Update the parsed analysis with curation readiness
                parsed_analysis["curation_ready"] = curation_ready
                parsed_analysis["missing_fields"] = missing_fields
                
            except json.JSONDecodeError:
                parsed_analysis = {
                    "host_species": {"primary": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "JSON parsing failed", "suggestions_for_curation": "Re-run analysis"},
                    "body_site": {"site": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "JSON parsing failed", "suggestions_for_curation": "Re-run analysis"},
                    "condition": {"description": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "JSON parsing failed", "suggestions_for_curation": "Re-run analysis"},
                    "sequencing_type": {"method": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "JSON parsing failed", "suggestions_for_curation": "Re-run analysis"},
                    "taxa_level": {"level": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "JSON parsing failed", "suggestions_for_curation": "Re-run analysis"},
                    "sample_size": {"size": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "JSON parsing failed", "suggestions_for_curation": "Re-run analysis"},
                    "curation_ready": False,
                    "missing_fields": ["host_species", "body_site", "condition", "sequencing_type", "taxa_level", "sample_size"],
                    "curation_preparation_summary": "Analysis failed - re-run required"
                }
            
            curation_ready = parsed_analysis.get("curation_ready", False)
            confidence = analysis.get("confidence", 0.0)
            
            # Store analysis results in cache
            cache_manager.store_analysis_result(
                pmid, 
                parsed_analysis, 
                metadata, 
                "gemini_enhanced", 
                confidence, 
                curation_ready
            )
            
            results.append({
                "pmid": pmid,
                "metadata": cached_result["metadata"],
                "enhanced_analysis": parsed_analysis,
                "curation_ready": curation_ready,
                "timestamp": datetime.now().isoformat(),
                "source": "gemini_enhanced_analysis",
                "cached": False,
                "status": "success"
            })
            
        except Exception as e:
            logger.error(f"Error processing PMID {pmid}: {str(e)}")
            results.append({
                "pmid": pmid,
                "status": "error",
                "error": str(e)
            })
    
    return results

def get_paper_metadata_from_csv(pmid, csv_path='data/full_dump.csv'):
    with open(csv_path, newline='', encoding='utf-8') as csvfile:
        # Skip comment lines (starting with #) or skip first 2 lines
        while True:
            pos = csvfile.tell()
            line = csvfile.readline()
            if not line:
                return None  # End of file, not found
            if isinstance(line, str) and not line.startswith('#'):
                csvfile.seek(pos)
                break
        reader = csv.DictReader(csvfile)
        for row in reader:
            if row.get('PMID') == pmid:
                # Combine both group sample sizes if present
                group0 = row.get('Group 0 sample size', '')
                group1 = row.get('Group 1 sample size', '')
                sample_size = f"Group 0: {group0}, Group 1: {group1}" if group0 or group1 else ''
                return {
                    'pmid': row.get('PMID', ''),
                    'title': row.get('Title', ''),
                    'authors': row.get('Authors list', ''),
                    'journal': row.get('Journal', ''),
                    'year': row.get('Year', ''),
                    'host': row.get('Host species', ''),
                    'body_site': row.get('Body site', ''),
                    'condition': row.get('Condition', ''),
                    'sequencing_type': row.get('Sequencing type', ''),
                    'in_bugsigdb': row.get('In BugSigDB', ''),
                    'sample_size': sample_size,
                    'taxa_level': row.get('Taxa Level', ''),
                    'statistical_method': row.get('Statistical test', ''),
                    'doi': row.get('DOI', ''),
                    'publication_date': row.get('Publication Date', ''),
                    'signature_probability': row.get('Signature Probability', ''),
                }
    return None

@app.get("/list_pmids", tags=["Batch Processing"])
def list_pmids():
    """
    **Get list of all available PMIDs from the CSV database.**
    
    This endpoint retrieves all PubMed IDs available in the local CSV database.
    Useful for batch processing and data exploration.
    
    **Returns:**
    - List of PMIDs as strings
    
    **Note:** This endpoint reads from the local CSV file and may take time for large datasets.
    """
    pmids = []
    try:
        with open('data/full_dump.csv', newline='', encoding='utf-8') as csvfile:
            # Skip comment lines and header
            while True:
                pos = csvfile.tell()
                line = csvfile.readline()
                if not line:
                    break
                if isinstance(line, str) and not line.startswith('#'):
                    csvfile.seek(pos)
                    break
            import csv as pycsv
            reader = pycsv.DictReader(csvfile)
            for row in reader:
                pmid = row.get('PMID')
                if pmid and pmid != 'NA':
                    pmids.append(pmid)
    except Exception as e:
        return {"error": str(e)}
    return pmids

@app.get("/health", tags=["System"])
async def health_check():
    """Health check endpoint for Docker and load balancers"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "service": "BioAnalyzer - BugSigDB Curation Analysis",
        "version": "1.0.0"
    }

@app.get("/metrics", tags=["System"])
async def metrics():
    """Basic metrics endpoint for monitoring"""
    return {
        "uptime": "running",
        "requests_processed": 0,  # TODO: Implement request counting
        "active_connections": 0,  # TODO: Implement connection tracking
        "memory_usage": "N/A",    # TODO: Implement memory monitoring
        "cpu_usage": "N/A"        # TODO: Implement CPU monitoring
    }

@app.get("/enhanced_analysis/{pmid}", tags=["Paper Analysis"])
async def enhanced_analysis(pmid: str):
    """
    **Enhanced analysis endpoint for BugSigDB curation requirements.**
    
    This endpoint provides the same 6-field analysis as the main analyze endpoint but with additional
    caching and performance optimizations. It's the recommended endpoint for production use.
    
    **Parameters:**
    - `pmid`: PubMed ID of the paper to analyze
    
    **Returns:**
    - **enhanced_analysis**: Detailed analysis of the 6 essential fields
    - **curation_ready**: Boolean indicating if the paper is ready for curation
    - **metadata**: Paper metadata (title, abstract, authors, etc.)
    - **cached**: Boolean indicating if result was retrieved from cache
    
    **6 Essential Fields Analyzed:**
    1. **host_species**: Host organism being studied
    2. **body_site**: Microbiome sample collection location
    3. **condition**: Disease/treatment/exposure studied
    4. **sequencing_type**: Molecular method used
    5. **taxa_level**: Taxonomic level analyzed
    6. **sample_size**: Number of samples analyzed
    
    **Note:** This endpoint is functionally identical to `/analyze/{pmid}` but includes caching.
    """
    try:
        logger.info(f"=== Starting enhanced analysis for PMID: {pmid} ===")
        
        # Check cache first for analysis results
        cached_result = cache_manager.get_analysis_result(pmid)
        if cached_result and cache_manager.is_cache_valid(cached_result["timestamp"]):
            logger.info(f"Returning cached analysis for PMID: {pmid}")
            return {
                "pmid": pmid,
                "metadata": cached_result["metadata"],
                "enhanced_analysis": cached_result["analysis_data"],
                "curation_ready": cached_result["curation_ready"],
                "timestamp": cached_result["timestamp"],
                "source": cached_result["source"],
                "cached": True
            }
        
        # Get paper metadata
        metadata = retriever.get_paper_metadata(pmid)
        csv_metadata = get_paper_metadata_from_csv(pmid)
        
        if csv_metadata:
            metadata.update(csv_metadata)
        
        if not metadata:
            raise HTTPException(status_code=404, detail=f"Paper not found: {pmid}")
        
        # Store metadata in cache
        cache_manager.store_metadata(pmid, metadata, "pubmed")
        
        # Get full text if available
        full_text = ""
        try:
            full_text = retriever.get_pmc_fulltext(pmid)
            if isinstance(full_text, str):
                try:
                    soup = BeautifulSoup(full_text, 'lxml')
                    full_text = retriever._extract_text_from_pmc_xml(soup)
                except Exception as e:
                    logger.warning(f"Failed to parse PMC XML for PMID {pmid}: {str(e)}")
            
            # Store full text in cache
            if full_text:
                cache_manager.store_fulltext(pmid, full_text, "pmc")
                
        except Exception as e:
            logger.warning(f"Failed to retrieve PMC full text for PMID {pmid}: {str(e)}")
            full_text = ""
        
        # Create enhanced prompt for specific analysis
        enhanced_prompt = f"""
        Analyze this scientific paper for BugSigDB curation. Focus ONLY on these 6 essential fields:

        PAPER INFORMATION:
        Title: {metadata.get('title', '')}
        Abstract: {metadata.get('abstract', '')}
        Full Text: {full_text[:3000] if full_text else 'Not available'}

        REQUIRED ANALYSIS - ONLY THESE 6 FIELDS:
        1. HOST SPECIES: What is the host species? (e.g., Human, Mouse, Rat, etc.)
        2. BODY SITE: What is the body site habitat of the microbiome? (e.g., Gut, Oral, Skin, etc.)
        3. CONDITION: What condition, treatment, or exposure is being studied?
        4. SEQUENCING TYPE: What molecular method was used? (e.g., 16S, metagenomics, etc.)
        5. TAXA LEVEL: What taxonomic level was analyzed? (e.g., phylum, genus, species, etc.)
        6. SAMPLE SIZE: What is the number of samples analyzed?

        IMPORTANT ANALYSIS REQUIREMENTS:
        - For each field, determine if the information is PRESENT, PARTIALLY_PRESENT, or ABSENT
        - If PRESENT: Extract the specific value and provide high confidence (0.8-1.0)
        - If PARTIALLY_PRESENT: Extract what's available and provide medium confidence (0.4-0.7)
        - If ABSENT: Provide reason why it's missing and confidence 0.0
        - For missing fields, suggest what additional information would be needed for curation

        Please provide your analysis in this exact JSON format:
        {{
            "host_species": {{
                "primary": "species_name",
                "confidence": 0.0-1.0,
                "status": "PRESENT|PARTIALLY_PRESENT|ABSENT",
                "reason_if_missing": "explanation if absent",
                "suggestions_for_curation": "what additional info is needed"
            }},
            "body_site": {{
                "site": "site_name",
                "confidence": 0.0-1.0,
                "status": "PRESENT|PARTIALLY_PRESENT|ABSENT",
                "reason_if_missing": "explanation if absent",
                "suggestions_for_curation": "what additional info is needed"
            }},
            "condition": {{
                "description": "detailed_description",
                "confidence": 0.0-1.0,
                "status": "PRESENT|PARTIALLY_PRESENT|ABSENT",
                "reason_if_missing": "explanation if absent",
                "suggestions_for_curation": "what additional info is needed"
            }},
            "sequencing_type": {{
                "method": "method_name",
                "confidence": 0.0-1.0,
                "status": "PRESENT|PARTIALLY_PRESENT|ABSENT",
                "reason_if_missing": "explanation if absent",
                "suggestions_for_curation": "what additional info is needed"
            }},
            "taxa_level": {{
                "level": "taxonomic_level",
                "confidence": 0.0-1.0,
                "status": "PRESENT|PARTIALLY_PRESENT|ABSENT",
                "reason_if_missing": "explanation if absent",
                "suggestions_for_curation": "what additional info is needed"
            }},
            "sample_size": {{
                "size": "sample_count",
                "confidence": 0.0-1.0,
                "status": "PRESENT|PARTIALLY_PRESENT|ABSENT",
                "reason_if_missing": "explanation if absent",
                "suggestions_for_curation": "what additional info is needed"
            }},
            "curation_ready": true/false,
            "missing_fields": ["field1", "field2", ...],
            "curation_preparation_summary": "Overall assessment of what's needed for curation"
        }}

        CRITICAL INSTRUCTIONS: 
        - Focus ONLY on the 6 fields listed above
        - Do NOT include Factor-Based Analysis, Detailed Explanation, Specific Reasons, Examples and Evidence, Key Findings, Category Scores, or Analysis Confidence
        - For each missing field, provide specific reason and suggestions
        - Determine curation readiness based on having all 6 fields with status "PRESENT"
        - Return ONLY the JSON structure above
        """
        
        # Run enhanced analysis using Gemini
        try:
            analysis = await qa_system.analyze_paper_enhanced(enhanced_prompt)
            
            # Parse the JSON response from Gemini
            try:
                parsed_analysis = json.loads(analysis.get("key_findings", "{}"))
                
                # Validate that we have exactly the 6 required fields
                required_fields = ["host_species", "body_site", "condition", "sequencing_type", "taxa_level", "sample_size"]
                missing_fields = []
                
                for field in required_fields:
                    if field not in parsed_analysis or not parsed_analysis[field]:
                        missing_fields.append(field)
                        # Ensure the field exists with default structure
                        if field == "host_species":
                            parsed_analysis[field] = {"primary": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "Field not found in analysis", "suggestions_for_curation": "Review paper for host species information"}
                        elif field == "body_site":
                            parsed_analysis[field] = {"site": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "Field not found in analysis", "suggestions_for_curation": "Review paper for body site information"}
                        elif field == "condition":
                            parsed_analysis[field] = {"description": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "Field not found in analysis", "suggestions_for_curation": "Review paper for condition information"}
                        elif field == "sequencing_type":
                            parsed_analysis[field] = {"method": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "Field not found in analysis", "suggestions_for_curation": "Review paper for sequencing method information"}
                        elif field == "taxa_level":
                            parsed_analysis[field] = {"level": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "Field not found in analysis", "suggestions_for_curation": "Review paper for taxonomic level information"}
                        elif field == "sample_size":
                            parsed_analysis[field] = {"size": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "Field not found in analysis", "suggestions_for_curation": "Review paper for sample size information"}
                
                # Determine curation readiness based on having all 6 fields with status "PRESENT"
                curation_ready = len(missing_fields) == 0 and all(
                    parsed_analysis.get(field, {}).get("status") == "PRESENT" 
                    for field in required_fields
                )
                
                # Update the parsed analysis with curation readiness
                parsed_analysis["curation_ready"] = curation_ready
                parsed_analysis["missing_fields"] = missing_fields
                
            except json.JSONDecodeError:
                # Fallback if JSON parsing fails - ensure we have exactly the 6 fields
                parsed_analysis = {
                    "host_species": {"primary": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "JSON parsing failed", "suggestions_for_curation": "Re-run analysis"},
                    "body_site": {"site": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "JSON parsing failed", "suggestions_for_curation": "Re-run analysis"},
                    "condition": {"description": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "JSON parsing failed", "suggestions_for_curation": "Re-run analysis"},
                    "sequencing_type": {"method": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "JSON parsing failed", "suggestions_for_curation": "Re-run analysis"},
                    "taxa_level": {"level": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "JSON parsing failed", "suggestions_for_curation": "Re-run analysis"},
                    "sample_size": {"size": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "JSON parsing failed", "suggestions_for_curation": "Re-run analysis"},
                    "curation_ready": False,
                    "missing_fields": ["host_species", "body_site", "condition", "sequencing_type", "taxa_level", "sample_size"],
                    "curation_preparation_summary": "Analysis failed - re-run required"
                }
            
            # Store analysis results in cache
            cache_manager.store_analysis_result(
                pmid, 
                parsed_analysis, 
                metadata, 
                "gemini_enhanced", 
                analysis.get("confidence", 0.0), 
                curation_ready
            )
            
            # Compose the enhanced response
            response = {
                "pmid": pmid,
                "metadata": metadata,
                "enhanced_analysis": parsed_analysis,
                "curation_ready": curation_ready,
                "timestamp": datetime.now().isoformat(),
                "source": "gemini_enhanced_analysis",
                "cached": False
            }
            
            return response
            
        except Exception as e:
            logger.error(f"Error during enhanced analysis: {str(e)}")
            raise HTTPException(status_code=500, detail=f"Analysis failed: {str(e)}")
            
    except HTTPException as he:
        raise he
    except Exception as e:
        logger.error(f"Error in enhanced analysis endpoint: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Unexpected error: {str(e)}")

@app.post("/enhanced_analysis_batch", tags=["Batch Processing"])
async def enhanced_analysis_batch(pmids: List[str] = Body(...), max_concurrent: int = Query(5)):
    """
    **Enhanced batch analysis endpoint for multiple papers.**
    
    This endpoint provides the same 6-field analysis as the regular batch endpoint but with
    additional performance optimizations and caching. It's the recommended endpoint for batch processing.
    
    **Parameters:**
    - `pmids`: List of PubMed IDs to analyze (max 50)
    - `max_concurrent`: Maximum concurrent processing (default: 5)
    
    **Returns:**
    - **batch_results**: List of analysis results for each PMID
    - **summary**: Processing statistics including:
        - Total PMIDs processed
        - Success/error counts
        - Cache hit rates
        - Processing timestamps
    
    **Performance Features:**
    - Intelligent caching to avoid re-analysis
    - Concurrent processing for better throughput
    - Detailed performance metrics
    """
    try:
        if not pmids:
            raise HTTPException(status_code=400, detail="No PMIDs provided")
        
        if len(pmids) > 50:
            raise HTTPException(status_code=400, detail="Maximum 50 PMIDs allowed per batch")
        
        results = []
        cached_count = 0
        new_analysis_count = 0
        
        # Process PMIDs with caching
        for pmid in pmids:
            try:
                # Check cache first
                cached_result = cache_manager.get_analysis_result(pmid)
                if cached_result and cache_manager.is_cache_valid(cached_result["timestamp"]):
                    results.append({
                        "pmid": pmid,
                        "metadata": cached_result["metadata"],
                        "enhanced_analysis": cached_result["analysis_data"],
                        "curation_ready": cached_result["curation_ready"],
                        "timestamp": cached_result["timestamp"],
                        "source": cached_result["source"],
                        "cached": True,
                        "status": "success"
                    })
                    cached_count += 1
                else:
                    # Get metadata and run analysis
                    metadata = retriever.get_paper_metadata(pmid)
                    csv_metadata = get_paper_metadata_from_csv(pmid)
                    
                    if csv_metadata:
                        metadata.update(csv_metadata)
                    
                    if not metadata:
                        results.append({
                            "pmid": pmid,
                            "status": "error",
                            "error": "Paper not found"
                        })
                        continue
                    
                    # Store metadata in cache
                    cache_manager.store_metadata(pmid, metadata, "pubmed")
                    
                    # Get full text if available
                    full_text = ""
                    try:
                        full_text = retriever.get_pmc_fulltext(pmid)
                        if isinstance(full_text, str):
                            try:
                                soup = BeautifulSoup(full_text, 'lxml')
                                full_text = retriever._extract_text_from_pmc_xml(soup)
                            except Exception as e:
                                logger.warning(f"Failed to parse PMC XML for PMID {pmid}: {str(e)}")
                        
                        if full_text:
                            cache_manager.store_fulltext(pmid, full_text, "pmc")
                    except Exception as e:
                        logger.warning(f"Failed to retrieve PMC full text for PMID {pmid}: {str(e)}")
                    
                    # Run enhanced analysis with the same improved prompt
                    enhanced_prompt = f"""
                    Analyze this scientific paper for BugSigDB curation. Focus ONLY on these 6 essential fields:

                    PAPER INFORMATION:
                    Title: {metadata.get('title', '')}
                    Abstract: {metadata.get('abstract', '')}
                    Full Text: {full_text[:3000] if full_text else 'Not available'}

                    REQUIRED ANALYSIS - ONLY THESE 6 FIELDS:
                    1. HOST SPECIES: What is the host species? (e.g., Human, Mouse, Rat, etc.)
                    2. BODY SITE: What is the body site habitat of the microbiome? (e.g., Gut, Oral, Skin, etc.)
                    3. CONDITION: What condition, treatment, or exposure is being studied?
                    4. SEQUENCING TYPE: What molecular method was used? (e.g., 16S, metagenomics, etc.)
                    5. TAXA LEVEL: What taxonomic level was analyzed? (e.g., phylum, genus, species, etc.)
                    6. SAMPLE SIZE: What is the number of samples analyzed?

                    IMPORTANT ANALYSIS REQUIREMENTS:
                    - For each field, determine if the information is PRESENT, PARTIALLY_PRESENT, or ABSENT
                    - If PRESENT: Extract the specific value and provide high confidence (0.8-1.0)
                    - If PARTIALLY_PRESENT: Extract what's available and provide medium confidence (0.4-0.7)
                    - If ABSENT: Provide reason why it's missing and confidence 0.0
                    - For missing fields, suggest what additional information would be needed for curation

                    Please provide your analysis in this exact JSON format:
                    {{
                        "host_species": {{
                            "primary": "species_name",
                            "confidence": 0.0-1.0,
                            "status": "PRESENT|PARTIALLY_PRESENT|ABSENT",
                            "reason_if_missing": "explanation if absent",
                            "suggestions_for_curation": "what additional info is needed"
                        }},
                        "body_site": {{
                            "site": "site_name",
                            "confidence": 0.0-1.0,
                            "status": "PRESENT|PARTIALLY_PRESENT|ABSENT",
                            "reason_if_missing": "explanation if absent",
                            "suggestions_for_curation": "what additional info is needed"
                        }},
                        "condition": {{
                            "description": "detailed_description",
                            "confidence": 0.0-1.0,
                            "status": "PRESENT|PARTIALLY_PRESENT|ABSENT",
                            "reason_if_missing": "explanation if absent",
                            "suggestions_for_curation": "what additional info is needed"
                        }},
                        "sequencing_type": {{
                            "method": "method_name",
                            "confidence": 0.0-1.0,
                            "status": "PRESENT|PARTIALLY_PRESENT|ABSENT",
                            "reason_if_missing": "explanation if absent",
                            "suggestions_for_curation": "what additional info is needed"
                        }},
                        "taxa_level": {{
                            "level": "taxonomic_level",
                            "confidence": 0.0-1.0,
                            "status": "PRESENT|PARTIALLY_PRESENT|ABSENT",
                            "reason_if_missing": "explanation if absent",
                            "suggestions_for_curation": "what additional info is needed"
                        }},
                        "sample_size": {{
                            "size": "sample_count",
                            "confidence": 0.0-1.0,
                            "status": "PRESENT|PARTIALLY_PRESENT|ABSENT",
                            "reason_if_missing": "explanation if absent",
                            "suggestions_for_curation": "what additional info is needed"
                        }},
                        "curation_ready": true/false,
                        "missing_fields": ["field1", "field2", ...],
                        "curation_preparation_summary": "Overall assessment of what's needed for curation"
                    }}

                    CRITICAL INSTRUCTIONS: 
                    - Focus ONLY on the 6 fields listed above
                    - Do NOT include Factor-Based Analysis, Detailed Explanation, Specific Reasons, Examples and Evidence, Key Findings, Category Scores, or Analysis Confidence
                    - For each missing field, provide specific reason and suggestions
                    - Determine curation readiness based on having all 6 fields with status "PRESENT"
                    - Return ONLY the JSON structure above
                    """
                    
                    analysis = await qa_system.analyze_paper_enhanced(enhanced_prompt)
                    
                    try:
                        parsed_analysis = json.loads(analysis.get("key_findings", "{}"))
                        
                        # Validate that we have exactly the 6 required fields
                        required_fields = ["host_species", "body_site", "condition", "sequencing_type", "taxa_level", "sample_size"]
                        missing_fields = []
                        
                        for field in required_fields:
                            if field not in parsed_analysis or not parsed_analysis[field]:
                                missing_fields.append(field)
                                # Ensure the field exists with default structure
                                if field == "host_species":
                                    parsed_analysis[field] = {"primary": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "Field not found in analysis", "suggestions_for_curation": "Review paper for host species information"}
                                elif field == "body_site":
                                    parsed_analysis[field] = {"site": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "Field not found in analysis", "suggestions_for_curation": "Review paper for body site information"}
                                elif field == "condition":
                                    parsed_analysis[field] = {"description": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "Field not found in analysis", "suggestions_for_curation": "Review paper for condition information"}
                                elif field == "sequencing_type":
                                    parsed_analysis[field] = {"method": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "Field not found in analysis", "suggestions_for_curation": "Review paper for sequencing method information"}
                                elif field == "taxa_level":
                                    parsed_analysis[field] = {"level": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "Field not found in analysis", "suggestions_for_curation": "Review paper for taxonomic level information"}
                                elif field == "sample_size":
                                    parsed_analysis[field] = {"size": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "Field not found in analysis", "suggestions_for_curation": "Review paper for sample size information"}
                        
                        # Determine curation readiness based on having all 6 fields with status "PRESENT"
                        curation_ready = len(missing_fields) == 0 and all(
                            parsed_analysis.get(field, {}).get("status") == "PRESENT" 
                            for field in required_fields
                        )
                        
                        # Update the parsed analysis with curation readiness
                        parsed_analysis["curation_ready"] = curation_ready
                        parsed_analysis["missing_fields"] = missing_fields
                        
                    except json.JSONDecodeError:
                        parsed_analysis = {
                            "host_species": {"primary": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "JSON parsing failed", "suggestions_for_curation": "Re-run analysis"},
                            "body_site": {"site": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "JSON parsing failed", "suggestions_for_curation": "Re-run analysis"},
                            "condition": {"description": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "JSON parsing failed", "suggestions_for_curation": "Re-run analysis"},
                            "sequencing_type": {"method": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "JSON parsing failed", "suggestions_for_curation": "Re-run analysis"},
                            "taxa_level": {"level": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "JSON parsing failed", "suggestions_for_curation": "Re-run analysis"},
                            "sample_size": {"size": "Unknown", "confidence": 0.0, "status": "ABSENT", "reason_if_missing": "JSON parsing failed", "suggestions_for_curation": "Re-run analysis"},
                            "curation_ready": False,
                            "missing_fields": ["host_species", "body_site", "condition", "sequencing_type", "taxa_level", "sample_size"],
                            "curation_preparation_summary": "Analysis failed - re-run required"
                        }
                    
                    curation_ready = parsed_analysis.get("curation_ready", False)
                    confidence = analysis.get("confidence", 0.0)
                    
                    # Store analysis results in cache
                    cache_manager.store_analysis_result(
                        pmid, 
                        parsed_analysis, 
                        metadata, 
                        "gemini_enhanced", 
                        confidence, 
                        curation_ready
                    )
                    
                    results.append({
                        "pmid": pmid,
                        "metadata": cached_result["metadata"],
                        "enhanced_analysis": parsed_analysis,
                        "curation_ready": curation_ready,
                        "timestamp": datetime.now().isoformat(),
                        "source": "gemini_enhanced_analysis",
                        "cached": False,
                        "status": "success"
                    })
                    new_analysis_count += 1
                    
            except Exception as e:
                logger.error(f"Error processing PMID {pmid}: {str(e)}")
                results.append({
                    "pmid": pmid,
                    "status": "error",
                    "error": str(e)
                })
        
        return {
            "batch_results": results,
            "summary": {
                "total_pmids": len(pmids),
                "successful": len([r for r in results if r.get("status") == "success"]),
                "errors": len([r for r in results if r.get("status") == "error"]),
                "cached_results": cached_count,
                "new_analysis": new_analysis_count,
                "timestamp": datetime.now().isoformat()
            }
        }
        
    except Exception as e:
        logger.error(f"Error in batch enhanced analysis: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Batch analysis failed: {str(e)}")

@app.get("/cache/stats", tags=["Cache Management"])
async def get_cache_stats():
    """Get cache statistics and information."""
    try:
        stats = cache_manager.get_cache_stats()
        return {
            "cache_stats": stats,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Error getting cache stats: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to get cache stats: {str(e)}")

@app.post("/cache/clear", tags=["Cache Management"])
async def clear_old_cache(max_age_hours: int = 168):
    """Clear old cache entries. Default: clear entries older than 1 week."""
    try:
        cleared_count = cache_manager.clear_old_cache(max_age_hours)
        return {
            "message": f"Cleared {cleared_count} old cache entries",
            "cleared_count": cleared_count,
            "max_age_hours": max_age_hours,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Error clearing cache: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to clear cache: {str(e)}")

@app.get("/cache/search", tags=["Cache Management"])
async def search_cache(query: str, search_type: str = "all"):
    """Search cache for papers matching the query."""
    try:
        results = cache_manager.search_cache(query, search_type)
        return {
            "query": query,
            "search_type": search_type,
            "results": results,
            "result_count": len(results),
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"Error searching cache: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to search cache: {str(e)}")

@app.get("/curation/statistics", tags=["Curation Statistics"])
async def get_curation_statistics():
    """
    **Get statistics about curation readiness for the 6 essential fields.**
    
    This endpoint provides comprehensive metrics about how well the AI analysis is performing
    in identifying the 6 essential BugSigDB curation fields across all analyzed papers.
    
    **Returns:**
    - **overall_statistics**: 
        - Total papers analyzed
        - Papers ready for curation
        - Overall readiness rate
    - **field_statistics**: Performance metrics for each of the 6 fields:
        - **host_species**: Readiness rate and confidence scores
        - **body_site**: Readiness rate and confidence scores
        - **condition**: Readiness rate and confidence scores
        - **sequencing_type**: Readiness rate and confidence scores
        - **taxa_level**: Readiness rate and confidence scores
        - **sample_size**: Readiness rate and confidence scores
    
    **Use Cases:**
    - Monitor AI analysis performance
    - Identify which fields are most challenging
    - Track curation readiness trends
    - Quality assurance and improvement
    """
    try:
        # Get all cached analysis results
        stats = cache_manager.get_cache_stats()
        
        # Analyze the 6 essential fields across all cached results
        field_stats = {
            "host_species": {"ready": 0, "total": 0, "avg_confidence": 0.0},
            "body_site": {"ready": 0, "total": 0, "avg_confidence": 0.0},
            "condition": {"ready": 0, "total": 0, "avg_confidence": 0.0},
            "sequencing_type": {"ready": 0, "total": 0, "avg_confidence": 0.0},
            "taxa_level": {"ready": 0, "total": 0, "avg_confidence": 0.0},
            "sample_size": {"ready": 0, "total": 0, "avg_confidence": 0.0}
        }
        
        # Get all analysis results from cache
        all_results = cache_manager.get_all_analysis_results()
        
        for result in all_results:
            analysis = result.get("analysis_data", {})
            for field_name in field_stats.keys():
                field_data = analysis.get(field_name, {})
                if field_data and field_data.get("confidence", 0.0) > 0.0:
                    field_stats[field_name]["ready"] += 1
                    field_stats[field_name]["avg_confidence"] += field_data.get("confidence", 0.0)
                field_stats[field_name]["total"] += 1
        
        # Calculate averages
        for field_name in field_stats.keys():
            if field_stats[field_name]["total"] > 0:
                field_stats[field_name]["avg_confidence"] /= field_stats[field_name]["total"]
                field_stats[field_name]["readiness_rate"] = (
                    field_stats[field_name]["ready"] / field_stats[field_name]["total"]
                )
            else:
                field_stats[field_name]["readiness_rate"] = 0.0
        
        # Overall curation readiness
        total_papers = len(all_results)
        ready_papers = len([r for r in all_results if r.get("curation_ready", False)])
        
        return {
            "overall_statistics": {
                "total_papers_analyzed": total_papers,
                "papers_ready_for_curation": ready_papers,
                "overall_readiness_rate": ready_papers / total_papers if total_papers > 0 else 0.0
            },
            "field_statistics": field_stats,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Error getting curation statistics: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Failed to get curation statistics: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    print("Starting BugSigDB Analyzer API...")
    uvicorn.run(app, host="127.0.0.1", port=8000, log_level="info")